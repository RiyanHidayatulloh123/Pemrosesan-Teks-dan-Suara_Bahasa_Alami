{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyfUgnK_Opt-",
        "outputId": "a3f1fb0f-6e6e-4305-b18e-7cf844e47e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp 'kaggle.json' ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/'kaggle.json'\n",
        "!kaggle datasets download -d 'jp797498e/twitter-entity-sentiment-analysis'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "n_ilO01vOzXm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_hot_encoded(label):\n",
        "  if label.lower() == 'positive':\n",
        "    return [1, 0, 0]\n",
        "  elif label.lower() == 'negative':\n",
        "    return [0, 1, 0]\n",
        "  else:\n",
        "    return [0, 0, 1]"
      ],
      "metadata": {
        "id": "pDYMWwFZO_04"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url (text):\n",
        "  # http://google.com adalah bla bla bla\n",
        "  # https://youtube.com\n",
        "  return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_number(text):\n",
        "  # 123 saya blablabla\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  words = text.split() # saya mau ke batam -> [saya, mau, ke, batam]\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    if word not in stopwords.words('english'):\n",
        "      new_text += word\n",
        "      new_text += \" \"\n",
        "\n",
        "  return new_text\n",
        "\n",
        "def lematized_text(text):\n",
        "  lematizer = WordNetLemmatizer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    lematized_word = lematizer.lemmatize(word)\n",
        "    new_text += lematized_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def stemmed_text(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    new_text += stemmed_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def preprocessing (text):\n",
        "  #lowercase -> Running, running, RUNNING, RunNinG\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove url\n",
        "  text = remove_url(text)\n",
        "\n",
        "  # remove number\n",
        "  text = remove_number(text)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = remove_stopwords(text)\n",
        "\n",
        "  # lematisasi\n",
        "  text = lematized_text(text)\n",
        "\n",
        "  # stemming\n",
        "  text = stemmed_text(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "N-xoYtGFPGiK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"ini bapak budi\" -> 3 kata\n",
        "# \"ini ibu budi\" -> 3 kata\n",
        "# \"saya dan bapak ibu budi pergi ke pasar\" -> 8 kata\n",
        "# 0, 0, 0 , 0, 0\n",
        "\n",
        "def get_avg (text_list):\n",
        "  sum = 0\n",
        "  for text in text_list:\n",
        "    word_num = len(text)\n",
        "    sum = sum + word_num\n",
        "  return sum/len(text_list) # total kata di dataset / banyak kalimat yang ada"
      ],
      "metadata": {
        "id": "9SSA471NPVQU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def tokenizing(text_list):\n",
        "  VOCAB_SIZE = 10000\n",
        "  tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token='<OOV>')\n",
        "  tokenizer.fit_on_texts(text_list)\n",
        "  word_index = tokenizer.word_index\n",
        "  max_length = get_avg(text_list)\n",
        "  print(max_length)\n",
        "  max_length = int(max_length)\n",
        "  print(max_length)\n",
        "  sequences = tokenizer.texts_to_sequences(text_list)\n",
        "  padded_sequences = pad_sequences(sequences, padding='pre', truncating='pre', maxlen=max_length)\n",
        "\n",
        "  with open('mytokenizer.pickle', 'wb') as file:\n",
        "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return padded_sequences"
      ],
      "metadata": {
        "id": "Re4i93AUPW6c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "eMsbAHdrPdPw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=10001, output_dim=100, input_length=57),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(100, activation = 'relu', input_shape=(None, 75)),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "61AqALv1PmxF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy'],\n",
        "    optimizer = 'adam'\n",
        ")"
      ],
      "metadata": {
        "id": "Zsr8BASQPqR6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mymodel.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2mZI2LFPt3d",
        "outputId": "128a7d76-84f1-4a74-9ba3-f6c7c38142dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}